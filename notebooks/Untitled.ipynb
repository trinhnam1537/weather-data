{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2b29fc-40ba-4ae2-9ac5-7376b0135ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, split, to_date, concat_ws,date_format\n",
    "from pyspark.sql.functions import regexp_replace, split, col, trim,when,lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19118875-26fa-41c1-bf8d-c3c939610dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read HDFS Weather Data\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c041a2-c0ac-4d99-9eb8-0717c01d2575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+-----+-------+-----+--------+------+-------+\n",
      "|      Date| Time|             Weather| Temp|   Rain|Cloud|Pressure|  Wind|   Gust|\n",
      "+----------+-----+--------------------+-----+-------+-----+--------+------+-------+\n",
      "|2010-01-01|00:00|Patchy rain possible|18 °c|0.2\\nmm| 100%| 1015 mb|3 km/h| 5 km/h|\n",
      "|2010-01-01|03:00|Patchy rain possible|17 °c|0.3\\nmm| 100%| 1014 mb|5 km/h| 7 km/h|\n",
      "|2010-01-01|06:00|Patchy rain possible|17 °c|0.2\\nmm| 100%| 1015 mb|6 km/h| 8 km/h|\n",
      "|2010-01-01|09:00|            Overcast|17 °c|0.0\\nmm| 100%| 1017 mb|6 km/h|12 km/h|\n",
      "|2010-01-01|12:00|            Overcast|17 °c|0.0\\nmm|  99%| 1015 mb|6 km/h|12 km/h|\n",
      "|2010-01-01|15:00|Patchy rain possible|18 °c|0.1\\nmm|  99%| 1014 mb|6 km/h| 8 km/h|\n",
      "|2010-01-01|18:00|            Overcast|17 °c|0.0\\nmm|  99%| 1015 mb|3 km/h| 4 km/h|\n",
      "|2010-01-01|21:00|            Overcast|17 °c|0.0\\nmm| 100%| 1015 mb|4 km/h|10 km/h|\n",
      "|2010-01-02|00:00|Patchy rain possible|17 °c|0.3\\nmm| 100%| 1016 mb|6 km/h|10 km/h|\n",
      "|2010-01-02|03:00|            Overcast|17 °c|0.0\\nmm| 100%| 1014 mb|5 km/h| 7 km/h|\n",
      "|2010-01-02|06:00|Patchy rain possible|17 °c|0.1\\nmm| 100%| 1015 mb|6 km/h| 9 km/h|\n",
      "|2010-01-02|09:00|Patchy rain possible|17 °c|0.3\\nmm| 100%| 1018 mb|8 km/h|15 km/h|\n",
      "|2010-01-02|12:00|            Overcast|18 °c|0.0\\nmm|  94%| 1016 mb|6 km/h|15 km/h|\n",
      "|2010-01-02|15:00|            Overcast|19 °c|0.0\\nmm|  87%| 1014 mb|5 km/h|13 km/h|\n",
      "|2010-01-02|18:00|              Cloudy|18 °c|0.0\\nmm|  78%| 1015 mb|5 km/h|11 km/h|\n",
      "|2010-01-02|21:00|            Overcast|18 °c|0.0\\nmm|  97%| 1018 mb|2 km/h| 3 km/h|\n",
      "|2010-01-03|00:00|            Overcast|17 °c|0.0\\nmm|  96%| 1018 mb|4 km/h| 7 km/h|\n",
      "|2010-01-03|03:00|            Overcast|17 °c|0.0\\nmm|  89%| 1017 mb|4 km/h| 6 km/h|\n",
      "|2010-01-03|06:00|            Overcast|17 °c|0.0\\nmm|  86%| 1018 mb|6 km/h| 9 km/h|\n",
      "|2010-01-03|09:00|            Overcast|18 °c|0.0\\nmm|  90%| 1018 mb|3 km/h| 9 km/h|\n",
      "+----------+-----+--------------------+-----+-------+-----+--------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hanoi = spark.read.option(\"multiLine\", True)\\\n",
    "               .option(\"header\", True)\\\n",
    "               .option(\"inferSchema\", False)\\\n",
    "               .option(\"encoding\", \"utf-8\")\\\n",
    "               .csv(\"hdfs://namenode:9000/tmp/weather_data/history/ha-noi.csv\")\n",
    "df_hanoi.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3245c15-e5fb-494d-81ef-eaa69153c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- Temp: string (nullable = true)\n",
      " |-- Rain: string (nullable = true)\n",
      " |-- Cloud: string (nullable = true)\n",
      " |-- Pressure: string (nullable = true)\n",
      " |-- Wind: string (nullable = true)\n",
      " |-- Gust: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hanoi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae62f5e-c674-4781-bbb0-dea889c8216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df):\n",
    "    df_clean=df.withColumn(\"Date\",to_date(col(\"Date\"),\"yyyy-MM-dd\")) \\\n",
    "               .withColumn(\"Time\",date_format(col(\"Time\"),,\"HH:mm\")) \\\n",
    "               .withColumn(\"Temp(°c)\",regexp_replace(\"Temp\",\"°c\",\"\").cast(\"double\")) \\\n",
    "               .withColumn(\"Rain(mm)\", regexp_replace(\"Rain\", \"mm\", \"\").cast(\"double\")) \\\n",
    "               .withColumn(\"Cloud\", (regexp_replace(\"Cloud\", \"%\", \"\").cast(\"double\") )) \\\n",
    "               .withColumn(\"Pressure(mb)\", regexp_replace(\"Pressure\", \"mb\", \"\").cast(\"double\")) \\\n",
    "               .withColumn(\"Wind(km/h)\", regexp_replace(\"Wind\", \"km/h\", \"\").cast(\"double\")) \\\n",
    "               .withColumn(\"Gust(km/h)\", regexp_replace(\"Gust\", \"km/h\", \"\").cast(\"double\")) \n",
    "    df_clean=df_clean.drop(\"Time\",\"Temp\",\"Rain\",\"Cloud\",\"Pressure\",\"Wind\",\"Gust\")\n",
    "    \n",
    "    weather_type1 = ['Sunny', 'Clear', 'Partly cloudy']\n",
    "    weather_type2 = ['Overcast', 'Cloudy', 'Patchy rain possible', 'Light drizzle', 'Light rain shower', 'Patchy light rain with thunder']\n",
    "    weather_type3 = ['Heavy rain at times', 'Moderate or heavy rain shower', 'Moderate rain at times', 'Moderate rain']\n",
    "    df_mapped = df_clean.withColumn(\n",
    "        \"Weather\",\n",
    "         when(col(\"Weather\").isin(weather_type1), 1)\n",
    "        .when(col(\"Weather\").isin(weather_type2), 2)\n",
    "        .when(col(\"Weather\").isin(weather_type3), 3)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    window_spec = Window.orderBy(\"Date\",\"time\")\n",
    "    lag_steps=4\n",
    "    df_lag=df_mapped\n",
    "    for lag_step in range(1, lag_steps + 1):\n",
    "        df_lag = df_lag \\\n",
    "            .withColumn(f\"Temp_t-{lag_step}\", lag(\"Temp(°c)\", lag_step).over(window_spec)) \\\n",
    "            .withColumn(f\"Rain_t-{lag_step}\", lag(\"Rain(mm)\", lag_step).over(window_spec)) \\\n",
    "            .withColumn(f\"Cloud_t-{lag_step}\", lag(\"Cloud\", lag_step).over(window_spec)) \\\n",
    "            .withColumn(f\"Pressure_t-{lag_step}\", lag(\"Pressure(mb)\", lag_step).over(window_spec)) \\\n",
    "            .withColumn(f\"Wind_t-{lag_step}\", lag(\"Wind(km/h)\", lag_step).over(window_spec)) \\\n",
    "            .withColumn(f\"Gust_t-{lag_step}\", lag(\"Gust(km/h)\", lag_step).over(window_spec))\n",
    "    df_final = df.drop(\"Date\",\"Time\")\n",
    "    X= df_final.drop(\"Weather\")\n",
    "    Y=df_final.select(\"Weather\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b346e2d3-de37-4502-b297-31f5e9fdd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, lag, when, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def data_preprocess(df):\n",
    "    \n",
    "    \n",
    "    # Remove units and convert to numeric values\n",
    "    df = df.withColumn(\"Temp\", trim(regexp_replace(col(\"Temp\"), \"°c\", \"\")))\n",
    "    df = df.withColumn(\"Rain\", trim(regexp_replace(col(\"Rain\"), \"\\nmm\", \"\")))\n",
    "    df = df.withColumn(\"Cloud\", trim(regexp_replace(col(\"Cloud\"), \"%\", \"\")))\n",
    "    df = df.withColumn(\"Pressure\", trim(regexp_replace(col(\"Pressure\"), \"mb\", \"\")))\n",
    "    df = df.withColumn(\"Wind\", trim(regexp_replace(col(\"Wind\"), \"km/h\", \"\")))\n",
    "    df = df.withColumn(\"Gust\", trim(regexp_replace(col(\"Gust\"), \"km/h\", \"\")))\n",
    "    \n",
    "    # Cast to proper data types\n",
    "    df = df.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
    "    df = df.withColumn(\"Temp\", col(\"Temp\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"Rain\", col(\"Rain\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"Cloud\", col(\"Cloud\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"Pressure\", col(\"Pressure\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"Wind\", col(\"Wind\").cast(DoubleType()))\n",
    "    df = df.withColumn(\"Gust\", col(\"Gust\").cast(DoubleType()))\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.withColumnRenamed(\"Temp\", \"Temp(°c)\")\n",
    "    df = df.withColumnRenamed(\"Rain\", \"Rain(nmm)\")\n",
    "    df = df.withColumnRenamed(\"Cloud\", \"Cloud(%)\")\n",
    "    df = df.withColumnRenamed(\"Pressure\", \"Pressure(mb)\")\n",
    "    df = df.withColumnRenamed(\"Wind\", \"Wind(km/h)\")\n",
    "    df = df.withColumnRenamed(\"Gust\", \"Gust(km/h)\")\n",
    "    \n",
    "    # Group weather types\n",
    "    weather_type1 = [\"Sunny\", \"Clear\", \"Partly cloudy\"]\n",
    "    weather_type2 = [\"Overcast\", \"Cloudy\", \"Patchy rain possible\", \"Light drizzle\", \"Light rain shower\", \"Patchy light rain with thunder\"]\n",
    "    weather_type3 = [\"Heavy rain at times\", \"Moderate or heavy rain shower\", \"Moderate rain at times\", \"Moderate rain\"]\n",
    "    \n",
    "    # Apply numeric encoding for Weather column\n",
    "    df = df.withColumn(\"Weather\", \n",
    "        when(col(\"Weather\").isin(weather_type1), 1)\n",
    "        .when(col(\"Weather\").isin(weather_type2), 2)\n",
    "        .when(col(\"Weather\").isin(weather_type3), 3)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Create window for lag features\n",
    "    window_spec = Window.orderBy(\"Date\", \"Time\")\n",
    "    \n",
    "    # Add lag features\n",
    "    lag_steps = 3\n",
    "    for lag_val in range(1, lag_steps + 1):\n",
    "        df = df.withColumn(f\"Temp_t-{lag_val}\", lag(col(\"Temp(°c)\"), lag_val).over(window_spec))\n",
    "        df = df.withColumn(f\"Rain_t-{lag_val}\", lag(col(\"Rain(nmm)\"), lag_val).over(window_spec))\n",
    "        df = df.withColumn(f\"Cloud_t-{lag_val}\", lag(col(\"Cloud(%)\"), lag_val).over(window_spec))\n",
    "        df = df.withColumn(f\"Pressure_t-{lag_val}\", lag(col(\"Pressure(mb)\"), lag_val).over(window_spec))\n",
    "        df = df.withColumn(f\"Wind_t-{lag_val}\", lag(col(\"Wind(km/h)\"), lag_val).over(window_spec))\n",
    "        df = df.withColumn(f\"Gust_t-{lag_val}\", lag(col(\"Gust(km/h)\"), lag_val).over(window_spec))\n",
    "    \n",
    "    # Drop Date and Time columns\n",
    "    df = df.drop(\"Date\", \"Time\")\n",
    "    \n",
    "    # Create X (features) and Y (target)\n",
    "    X = df.drop(\"Weather\")\n",
    "    \n",
    "    # Shift the Y for prediction (equivalent to Y[lag_steps + 1:])\n",
    "    window_spec_shift = Window.orderBy(F.monotonically_increasing_id())\n",
    "    Y = df.select(\"Weather\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    Y = Y.withColumn(\"Weather_shifted\", lag(col(\"Weather\"), -1).over(window_spec_shift))\n",
    "    Y = Y.filter(col(\"Weather_shifted\").isNotNull()).drop(\"Weather\").withColumnRenamed(\"Weather_shifted\", \"Weather\")\n",
    "    \n",
    "    # Filter X to remove NaN rows (equivalent to X[lag_steps:-1])\n",
    "    X = X.filter(col(f\"Temp_t-{lag_steps}\").isNotNull())\n",
    "    X = X.filter(F.monotonically_increasing_id() < F.lit(X.count() - 1))\n",
    "    \n",
    "    # Join X and Y\n",
    "    X = X.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    result = X.join(Y, \"id\").drop(\"id\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095a73d7-4629-48f1-9803-c5d20b26d85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+------------+----------+----------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+-------+\n",
      "|Temp(°c)|Rain(nmm)|Cloud(%)|Pressure(mb)|Wind(km/h)|Gust(km/h)|Temp_t-1|Rain_t-1|Cloud_t-1|Pressure_t-1|Wind_t-1|Gust_t-1|Temp_t-2|Rain_t-2|Cloud_t-2|Pressure_t-2|Wind_t-2|Gust_t-2|Temp_t-3|Rain_t-3|Cloud_t-3|Pressure_t-3|Wind_t-3|Gust_t-3|Weather|\n",
      "+--------+---------+--------+------------+----------+----------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+-------+\n",
      "|    17.0|      0.0|   100.0|      1017.0|       6.0|      12.0|    17.0|     0.2|    100.0|      1015.0|     6.0|     8.0|    17.0|     0.3|    100.0|      1014.0|     5.0|     7.0|    18.0|     0.2|    100.0|      1015.0|     3.0|     5.0|      2|\n",
      "|    17.0|      0.0|    99.0|      1015.0|       6.0|      12.0|    17.0|     0.0|    100.0|      1017.0|     6.0|    12.0|    17.0|     0.2|    100.0|      1015.0|     6.0|     8.0|    17.0|     0.3|    100.0|      1014.0|     5.0|     7.0|      2|\n",
      "|    18.0|      0.1|    99.0|      1014.0|       6.0|       8.0|    17.0|     0.0|     99.0|      1015.0|     6.0|    12.0|    17.0|     0.0|    100.0|      1017.0|     6.0|    12.0|    17.0|     0.2|    100.0|      1015.0|     6.0|     8.0|      2|\n",
      "|    17.0|      0.0|    99.0|      1015.0|       3.0|       4.0|    18.0|     0.1|     99.0|      1014.0|     6.0|     8.0|    17.0|     0.0|     99.0|      1015.0|     6.0|    12.0|    17.0|     0.0|    100.0|      1017.0|     6.0|    12.0|      2|\n",
      "|    17.0|      0.0|   100.0|      1015.0|       4.0|      10.0|    17.0|     0.0|     99.0|      1015.0|     3.0|     4.0|    18.0|     0.1|     99.0|      1014.0|     6.0|     8.0|    17.0|     0.0|     99.0|      1015.0|     6.0|    12.0|      2|\n",
      "|    17.0|      0.3|   100.0|      1016.0|       6.0|      10.0|    17.0|     0.0|    100.0|      1015.0|     4.0|    10.0|    17.0|     0.0|     99.0|      1015.0|     3.0|     4.0|    18.0|     0.1|     99.0|      1014.0|     6.0|     8.0|      2|\n",
      "|    17.0|      0.0|   100.0|      1014.0|       5.0|       7.0|    17.0|     0.3|    100.0|      1016.0|     6.0|    10.0|    17.0|     0.0|    100.0|      1015.0|     4.0|    10.0|    17.0|     0.0|     99.0|      1015.0|     3.0|     4.0|      2|\n",
      "|    17.0|      0.1|   100.0|      1015.0|       6.0|       9.0|    17.0|     0.0|    100.0|      1014.0|     5.0|     7.0|    17.0|     0.3|    100.0|      1016.0|     6.0|    10.0|    17.0|     0.0|    100.0|      1015.0|     4.0|    10.0|      2|\n",
      "|    17.0|      0.3|   100.0|      1018.0|       8.0|      15.0|    17.0|     0.1|    100.0|      1015.0|     6.0|     9.0|    17.0|     0.0|    100.0|      1014.0|     5.0|     7.0|    17.0|     0.3|    100.0|      1016.0|     6.0|    10.0|      2|\n",
      "|    18.0|      0.0|    94.0|      1016.0|       6.0|      15.0|    17.0|     0.3|    100.0|      1018.0|     8.0|    15.0|    17.0|     0.1|    100.0|      1015.0|     6.0|     9.0|    17.0|     0.0|    100.0|      1014.0|     5.0|     7.0|      2|\n",
      "|    19.0|      0.0|    87.0|      1014.0|       5.0|      13.0|    18.0|     0.0|     94.0|      1016.0|     6.0|    15.0|    17.0|     0.3|    100.0|      1018.0|     8.0|    15.0|    17.0|     0.1|    100.0|      1015.0|     6.0|     9.0|      2|\n",
      "|    18.0|      0.0|    78.0|      1015.0|       5.0|      11.0|    19.0|     0.0|     87.0|      1014.0|     5.0|    13.0|    18.0|     0.0|     94.0|      1016.0|     6.0|    15.0|    17.0|     0.3|    100.0|      1018.0|     8.0|    15.0|      2|\n",
      "|    18.0|      0.0|    97.0|      1018.0|       2.0|       3.0|    18.0|     0.0|     78.0|      1015.0|     5.0|    11.0|    19.0|     0.0|     87.0|      1014.0|     5.0|    13.0|    18.0|     0.0|     94.0|      1016.0|     6.0|    15.0|      2|\n",
      "|    17.0|      0.0|    96.0|      1018.0|       4.0|       7.0|    18.0|     0.0|     97.0|      1018.0|     2.0|     3.0|    18.0|     0.0|     78.0|      1015.0|     5.0|    11.0|    19.0|     0.0|     87.0|      1014.0|     5.0|    13.0|      2|\n",
      "|    17.0|      0.0|    89.0|      1017.0|       4.0|       6.0|    17.0|     0.0|     96.0|      1018.0|     4.0|     7.0|    18.0|     0.0|     97.0|      1018.0|     2.0|     3.0|    18.0|     0.0|     78.0|      1015.0|     5.0|    11.0|      2|\n",
      "|    17.0|      0.0|    86.0|      1018.0|       6.0|       9.0|    17.0|     0.0|     89.0|      1017.0|     4.0|     6.0|    17.0|     0.0|     96.0|      1018.0|     4.0|     7.0|    18.0|     0.0|     97.0|      1018.0|     2.0|     3.0|      2|\n",
      "|    18.0|      0.0|    90.0|      1018.0|       3.0|       9.0|    17.0|     0.0|     86.0|      1018.0|     6.0|     9.0|    17.0|     0.0|     89.0|      1017.0|     4.0|     6.0|    17.0|     0.0|     96.0|      1018.0|     4.0|     7.0|      2|\n",
      "|    19.0|      0.0|    80.0|      1016.0|       3.0|       8.0|    18.0|     0.0|     90.0|      1018.0|     3.0|     9.0|    17.0|     0.0|     86.0|      1018.0|     6.0|     9.0|    17.0|     0.0|     89.0|      1017.0|     4.0|     6.0|      2|\n",
      "|    20.0|      0.0|    70.0|      1014.0|       3.0|       3.0|    19.0|     0.0|     80.0|      1016.0|     3.0|     8.0|    18.0|     0.0|     90.0|      1018.0|     3.0|     9.0|    17.0|     0.0|     86.0|      1018.0|     6.0|     9.0|      2|\n",
      "|    18.0|      0.0|    50.0|      1014.0|       6.0|      10.0|    20.0|     0.0|     70.0|      1014.0|     3.0|     3.0|    19.0|     0.0|     80.0|      1016.0|     3.0|     8.0|    18.0|     0.0|     90.0|      1018.0|     3.0|     9.0|      2|\n",
      "|    18.0|      0.0|    51.0|      1015.0|       4.0|       6.0|    18.0|     0.0|     50.0|      1014.0|     6.0|    10.0|    20.0|     0.0|     70.0|      1014.0|     3.0|     3.0|    19.0|     0.0|     80.0|      1016.0|     3.0|     8.0|      2|\n",
      "|    17.0|      0.0|   100.0|      1013.0|       5.0|       8.0|    18.0|     0.0|     51.0|      1015.0|     4.0|     6.0|    18.0|     0.0|     50.0|      1014.0|     6.0|    10.0|    20.0|     0.0|     70.0|      1014.0|     3.0|     3.0|      1|\n",
      "|    18.0|      0.0|   100.0|      1012.0|       6.0|      13.0|    17.0|     0.0|    100.0|      1013.0|     5.0|     8.0|    18.0|     0.0|     51.0|      1015.0|     4.0|     6.0|    18.0|     0.0|     50.0|      1014.0|     6.0|    10.0|      1|\n",
      "|    18.0|      0.0|    98.0|      1013.0|       7.0|      11.0|    18.0|     0.0|    100.0|      1012.0|     6.0|    13.0|    17.0|     0.0|    100.0|      1013.0|     5.0|     8.0|    18.0|     0.0|     51.0|      1015.0|     4.0|     6.0|      2|\n",
      "|    21.0|      0.0|    84.0|      1013.0|       6.0|      13.0|    18.0|     0.0|     98.0|      1013.0|     7.0|    11.0|    18.0|     0.0|    100.0|      1012.0|     6.0|    13.0|    17.0|     0.0|    100.0|      1013.0|     5.0|     8.0|      2|\n",
      "|    23.0|      0.0|    76.0|      1011.0|       8.0|      16.0|    21.0|     0.0|     84.0|      1013.0|     6.0|    13.0|    18.0|     0.0|     98.0|      1013.0|     7.0|    11.0|    18.0|     0.0|    100.0|      1012.0|     6.0|    13.0|      2|\n",
      "|    25.0|      0.0|    68.0|      1008.0|      10.0|      14.0|    23.0|     0.0|     76.0|      1011.0|     8.0|    16.0|    21.0|     0.0|     84.0|      1013.0|     6.0|    13.0|    18.0|     0.0|     98.0|      1013.0|     7.0|    11.0|      2|\n",
      "|    21.0|      0.0|    34.0|      1009.0|      10.0|      17.0|    25.0|     0.0|     68.0|      1008.0|    10.0|    14.0|    23.0|     0.0|     76.0|      1011.0|     8.0|    16.0|    21.0|     0.0|     84.0|      1013.0|     6.0|    13.0|      2|\n",
      "|    19.0|      0.0|   100.0|      1010.0|       7.0|      16.0|    21.0|     0.0|     34.0|      1009.0|    10.0|    17.0|    25.0|     0.0|     68.0|      1008.0|    10.0|    14.0|    23.0|     0.0|     76.0|      1011.0|     8.0|    16.0|      2|\n",
      "|    19.0|      0.0|   100.0|      1009.0|       8.0|      14.0|    19.0|     0.0|    100.0|      1010.0|     7.0|    16.0|    21.0|     0.0|     34.0|      1009.0|    10.0|    17.0|    25.0|     0.0|     68.0|      1008.0|    10.0|    14.0|      1|\n",
      "+--------+---------+--------+------------+----------+----------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+--------+--------+---------+------------+--------+--------+-------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean=data_preprocess(df_hanoi)\n",
    "df_clean.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc0c920-5511-41b2-88f8-d47d04f5bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "def build_model(location,df):\n",
    "    # Khởi tạo Spark session\n",
    "    \n",
    "    \n",
    "    # Gọi hàm tiền xử lý trực tiếp từ Spark thay vì pandas\n",
    "    \n",
    "    \n",
    "    # Lấy tất cả các tên cột trừ cột Weather (có thể chỉnh sửa nếu cần)\n",
    "    feature_cols = [col_name for col_name in df.columns if col_name != \"Weather\"]\n",
    "    \n",
    "    # Chuyển dữ liệu về kiểu vector + standardize\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "    scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "    \n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier(labelCol=\"Weather\", featuresCol=\"features\")\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    \n",
    "    # Grid Search\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "        .addGrid(rf.numTrees, [50, 100])\n",
    "        .addGrid(rf.maxDepth, [5, 10, 20])\n",
    "        .addGrid(rf.minInstancesPerNode, [2, 5])\n",
    "        .build())\n",
    "    \n",
    "    # Evaluation & CV\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Weather\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5\n",
    "    )\n",
    "    \n",
    "    # Chia train/test\n",
    "    train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "    \n",
    "    # Fit model\n",
    "    print(f\"Training model for {location}...\")\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Dự đoán và đánh giá\n",
    "    print(f\"Evaluating model for {location}...\")\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Chi tiết metric\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"Weather\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    print(f\"Accuracy = {accuracy:.4f}\")\n",
    "    print(f\"Weighted Precision = {metrics.weightedPrecision:.4f}\")\n",
    "    print(f\"Weighted Recall = {metrics.weightedRecall:.4f}\")\n",
    "    print(f\"Weighted F1 Score = {metrics.weightedFMeasure():.4f}\")\n",
    "    model_path = \"hdfs://namenode:9000/tmp/model/random_model/\"\n",
    "    # Lưu model (có thể mở comment nếu cần)\n",
    "    cv_model.bestModel.write().overwrite().save(f\"{model_path}{location}_model\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deae0cec-8168-49ec-90e5-8ae9c7f3e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for ha-noi...\n",
      "Evaluating model for ha-noi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9977\n",
      "Weighted Precision = 0.9978\n",
      "Weighted Recall = 0.9977\n",
      "Weighted F1 Score = 0.9977\n",
      "0.9977465635093518\n"
     ]
    }
   ],
   "source": [
    "accu = build_model(\"ha-noi\",df_clean)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d83e79-e55a-4cc1-a844-3649ebcda4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b76b2-78f4-4fcc-8f0e-5669b0349a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
